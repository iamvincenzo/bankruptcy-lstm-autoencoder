{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "# SOME REFERENCES: https://medium.com/towards-data-science/whats-happening-in-my-lstm-layer-dd8110ecc52f #\n",
    "##########################################################################################################\n",
    "\n",
    "#################################################################################\n",
    "# SOME REFERENCES: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html #\n",
    "#################################################################################\n",
    "\n",
    "# For each layer in your LSTM — the number of cells is equal to the size of your window.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\" Class for a simple LSTM. \"\"\"\n",
    "class MyLSTM(nn.Module):\n",
    "    \"\"\" Initialize configurations. \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device, bidirectional=False):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        # number of features of x\n",
    "        self.input_size = input_size\n",
    "        # number of features in the hidden state h\n",
    "        self.hidden_size = hidden_size\n",
    "        # number of recurrent layers\n",
    "        self.num_layers = num_layers\n",
    "        # number of output neurons of linear layer\n",
    "        self.output_size = output_size\n",
    "        # D parameter\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        # device\n",
    "        self.device = device\n",
    "\n",
    "        # lstm-architecture:\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True).to(device)\n",
    "\n",
    "        # linear-layer\n",
    "        self.fc1 = nn.Sequential(\n",
    "            # we take the last cell outputs with shape (batch_size, D * hidden_size)\n",
    "            nn.Linear(self.directions * self.hidden_size, self.output_size),\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    \"\"\" Method used to define the forward pass of the input through the network during the training. \"\"\"\n",
    "    def forward(self, x):\n",
    "        # input shape (batch_size, sequence_length, number_features) when batch_first=True\n",
    "        batch_size = x.size(0)\n",
    "        # (D ∗ num_layers, batch_size, hidden_size)\n",
    "        h_0 = torch.zeros((self.directions * self.num_layers,\n",
    "                           batch_size, self.hidden_size)).to(self.device)\n",
    "        c_0 = torch.zeros((self.directions * self.num_layers,\n",
    "                           batch_size, self.hidden_size)).to(self.device)\n",
    "\n",
    "        # output-shape (batch_size, sequence_lenght, D * hidden_size)\n",
    "        # h_n-shape (D * num_layers, batch_size, hidden_size)\n",
    "        # c_n-shape (D * num_layers, batch_size, hidden_size)\n",
    "        output, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "\n",
    "        print(\"\\n\", output.shape, h_n.shape, c_n.shape)\n",
    "        print(\"\\n\", output, \"\\n\\n\", h_n, \"\\n\\n\", c_n)\n",
    "\n",
    "        # takes the last cell outputs of all batches\n",
    "        print(\"\\nlast cell outputs shape: \\n\", output[:, -1, :].shape)\n",
    "        print(\"\\nlast cell outputs: \\n\", output[:, -1, :])\n",
    "\n",
    "        lin_out = self.fc1(output[:, -1, :])\n",
    "\n",
    "        print(f\"\\nlin-out-shape: {lin_out}\")\n",
    "\n",
    "\n",
    "\"\"\" Runs the simulation. \"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    x = torch.randn((2, 6, 6))\n",
    "    print(f\"\\nx-shape: {x.shape}\")\n",
    "\n",
    "    num_features = int(x.shape[2])\n",
    "\n",
    "    model = MyLSTM(input_size=num_features,\n",
    "                   hidden_size=6,\n",
    "                   num_layers=1,\n",
    "                   output_size=1,\n",
    "                   device=device)\n",
    "\n",
    "    out = model(x)\n",
    "\n",
    "##################\n",
    "# output example #\n",
    "##################\n",
    "\n",
    "# device: cpu\n",
    "\n",
    "# x-shape: torch.Size([2, 6, 6])\n",
    "\n",
    "# torch.Size([2, 6, 6]) torch.Size([1, 2, 6]) torch.Size([1, 2, 6])\n",
    "\n",
    "# tensor([[[ 0.0670,  0.0398,  0.2406, -0.2882,  0.0184, -0.1168],\n",
    "#          [-0.0813,  0.2598,  0.3303, -0.1901,  0.1681, -0.1573],\n",
    "#          [-0.0013,  0.0993,  0.3154, -0.2576,  0.2350, -0.2473],\n",
    "#          [ 0.1037,  0.0869,  0.3699, -0.1835,  0.0433, -0.3695],\n",
    "#          [ 0.1005,  0.2060,  0.3575, -0.1126,  0.0005, -0.3399],\n",
    "#          [ 0.0111,  0.1685,  0.4348, -0.2917,  0.1025, -0.2308]],\n",
    "\n",
    "#         [[ 0.2161, -0.0216,  0.1619, -0.0560, -0.1214, -0.2652],\n",
    "#          [ 0.1342,  0.1155,  0.3858, -0.1652, -0.1072, -0.2497],\n",
    "#          [-0.0412,  0.2684,  0.1980, -0.1152,  0.2417, -0.0435],\n",
    "#          [-0.4885,  0.4563, -0.0637, -0.1715,  0.2013,  0.0407],\n",
    "#          [-0.1746,  0.2125,  0.0317, -0.2900,  0.3504, -0.0428],\n",
    "#          [-0.3115,  0.3664, -0.0838, -0.2712,  0.2742, -0.0829]]],\n",
    "#        grad_fn=<TransposeBackward0>)\n",
    "\n",
    "# tensor([[[ 0.0111,  0.1685,  0.4348, -0.2917,  0.1025, -0.2308],\n",
    "#          [-0.3115,  0.3664, -0.0838, -0.2712,  0.2742, -0.0829]]],\n",
    "#        grad_fn=<StackBackward0>)\n",
    "\n",
    "# tensor([[[ 0.0251,  0.3550,  0.9562, -0.6240,  0.2229, -0.6161],\n",
    "#          [-0.4754,  0.7592, -0.2685, -0.7482,  0.9308, -0.2028]]],\n",
    "#        grad_fn=<StackBackward0>)\n",
    "\n",
    "# last cell outputs shape: torch.Size([2, 6])\n",
    "\n",
    "# last cell outputs:\n",
    "# tensor([[ 0.0111,  0.1685,  0.4348, -0.2917,  0.1025, -0.2308],\n",
    "#         [-0.3115,  0.3664, -0.0838, -0.2712,  0.2742, -0.0829]],\n",
    "#        grad_fn=<SliceBackward0>)\n",
    "\n",
    "# lin-out-shape:\n",
    "# tensor([[0.2285],\n",
    "#         [0.3122]], grad_fn=<ReluBackward0>)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################\n",
    "# SOME REFERENCES: https://medium.com/towards-data-science/whats-happening-in-my-lstm-layer-dd8110ecc52f #\n",
    "##########################################################################################################\n",
    "\n",
    "#################################################################################\n",
    "# SOME REFERENCES: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html #\n",
    "#################################################################################\n",
    "\n",
    "# For each layer in your LSTM — the number of cells is equal to the size of your window.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\" Class form LSTM in Autoencoder configuration. \"\"\"\n",
    "class EncoderDecoderLSTM(nn.Module):\n",
    "    \"\"\" Initialize configurations. \"\"\"\n",
    "    def __init__(self, enc_input_size, dec_input_size, enc_hidden_size, dec_hidden_size, num_layers, device, bidirectional=False):\n",
    "        super(EncoderDecoderLSTM, self).__init__()\n",
    "        # the number of expected features in the input x\n",
    "        self.enc_input_size = enc_input_size\n",
    "        # the number of expected features in the output of the encoder\n",
    "        self.dec_input_size = dec_input_size\n",
    "        # the number of features in the hidden state h of the encoder\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        # the number of features in the hidden state h of the decoder\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        # number of recurrent layers\n",
    "        self.num_layers = num_layers\n",
    "        # if true becomes a bidirectional LSTM\n",
    "        D = 2 if bidirectional else 1\n",
    "        self.directions = D\n",
    "        # device where to put tensors\n",
    "        self.device = device\n",
    "\n",
    "        # lstm-architecture\n",
    "        self.lstm_encoder = nn.LSTM(input_size=enc_input_size,\n",
    "                                    hidden_size=enc_hidden_size,\n",
    "                                    num_layers=num_layers,\n",
    "                                    batch_first=True)\n",
    "        # lstm-architecture\n",
    "        self.lstm_decoder = nn.LSTM(input_size=dec_input_size,\n",
    "                                    hidden_size=dec_hidden_size,\n",
    "                                    num_layers=num_layers,\n",
    "                                    batch_first=True)\n",
    "\n",
    "    \"\"\" Method used to define the forward pass of the input through the network during the training. \"\"\"\n",
    "    def forward(self, x):\n",
    "        # # ENCODER \n",
    "        \"\"\" input shape (batch_size, sequence_length, number_features) when batch_first=True \"\"\"\n",
    "        # print(f\"\\ninput-shape: \\n{x.shape}\")\n",
    "        # print(f\"\\ninput: \\n{x}\")\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \"\"\" (D ∗ num_layers, batch_size, hidden_size) \"\"\"\n",
    "        h_0 = torch.zeros((self.directions * self.num_layers,\n",
    "                           batch_size, self.enc_hidden_size)).to(self.device)\n",
    "        c_0 = torch.zeros((self.directions * self.num_layers,\n",
    "                           batch_size, self.enc_hidden_size)).to(self.device)\n",
    "\n",
    "        \"\"\" output-shape (batch_size, sequence_lenght, D * hidden_size)\n",
    "        h_n-shape (D * num_layers, batch_size, hidden_size)\n",
    "        c_n-shape (D * num_layers, batch_size, hidden_size) \"\"\"\n",
    "        enc_output, (h_n, c_n) = self.lstm_encoder(x, (h_0, c_0))\n",
    "\n",
    "        # print(f\"\\nh_n-shape: \\n{h_n.shape}\")\n",
    "        # print(f\"\\nh_n: \\n{h_n}\")\n",
    "        # print(f\"\\nc_n-shape: \\n{c_n.shape}\")\n",
    "        # print(f\"\\nc_n: \\n{c_n}\")\n",
    "\n",
    "        # # DECODER\n",
    "        # print(f\"\\nenc-output-shape: \\n{enc_output.shape}\")\n",
    "        # print(f\"\\nenc-output: \\n{enc_output}\")\n",
    "\n",
    "        batch_size = enc_output.size(0)\n",
    "        \"\"\" (D ∗ num_layers, batch_size, hidden_size) \"\"\"\n",
    "        h_0 = torch.zeros((self.directions * self.num_layers,\n",
    "                           batch_size, self.dec_hidden_size)).to(self.device)\n",
    "        c_0 = torch.zeros((self.directions * self.num_layers,\n",
    "                           batch_size, self.dec_hidden_size)).to(self.device)\n",
    "\n",
    "        \"\"\" output-shape (batch_size, sequence_lenght, D * hidden_size)\n",
    "        h_n-shape (D * num_layers, batch_size, hidden_size)\n",
    "        c_n-shape (D * num_layers, batch_size, hidden_size) \"\"\"\n",
    "        dec_output, (_, _) = self.lstm_decoder(enc_output, (h_0, c_0))\n",
    "\n",
    "        # print(f\"\\ndec-output-shape: \\n{dec_output.shape}\")\n",
    "        # print(f\"\\ndec-output: \\n{dec_output}\")\n",
    "\n",
    "        return enc_output, dec_output\n",
    "\n",
    "\n",
    "\"\"\" Runs the simulation. \"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\ndevice: \\n{device}\")\n",
    "\n",
    "    batch_size = 1 # 2\n",
    "    seq_len = 5\n",
    "    enc_input_size = 18\n",
    "    dec_input_size = 5\n",
    "    enc_hidden_size = 5\n",
    "    dec_hidden_size = 18\n",
    "    num_layers = 1\n",
    "    x = torch.rand((batch_size, seq_len, num_features))\n",
    "\n",
    "    # model definition\n",
    "    model = EncoderDecoderLSTM(enc_input_size=enc_input_size, \n",
    "                               dec_input_size= dec_input_size,\n",
    "                               enc_hidden_size=enc_hidden_size, \n",
    "                               dec_hidden_size=dec_hidden_size, \n",
    "                               num_layers=num_layers, \n",
    "                               device=device)        \n",
    "    # model output\n",
    "    enc_output, dec_output = model(x)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # # other dimensions\n",
    "    # batch_size = 2  # 32\n",
    "    # seq_len = 5\n",
    "    # enc_input_size = 18\n",
    "    # dec_input_size = 18 #5\n",
    "    # enc_hidden_size = 18\n",
    "    # dec_hidden_size = 18\n",
    "    # num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Create the input tensors\n",
    "matrix = torch.rand(32, 5, 5)  # Shape: [32, 5, 5]\n",
    "vector = torch.rand(32, 5)  # Shape: [32, 5]\n",
    "\n",
    "# Expand dimensions of the vector to match the shape of the matrix\n",
    "expanded_vector = vector.unsqueeze(2)  # Shape: [32, 5, 1]\n",
    "\n",
    "# Perform the dot product using broadcasting: batched matrix x batched matrix\n",
    "result = torch.matmul(matrix, expanded_vector)  # Shape: [32, 5, 1]\n",
    "\n",
    "# Remove the extra dimension from the result\n",
    "result = result.squeeze(2)  # Shape: [32, 5]\n",
    "\n",
    "print(result.shape)  # Shape of the resulting tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
